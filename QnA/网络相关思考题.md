1、一端close(socket)时，发送FIN还是RST？发送RST会有什么问题？如何解决？

内核根据**接收缓冲**是否有数据，决定发送FIN还是RST。

https://z.itpub.net/article/detail/E07055E62740F8C59E1649519A7F1A41

当一端接收缓冲还有数据时调用了close，此时它会清空接收缓冲并向对端发送RST，这可能导致**对端接收缓冲**中的数据无法被顺利读取（？），导致对端程序出错。

https://www.starduster.me/2019/07/06/socket-lingering-and-closing/

↑使用SO_LINGER似乎不能解决？↑而且内核的实现还会导致非阻塞socket阻塞？Nginx是怎么处理lingering close的？（lingering close处理是怎么一回事？）





2、一端调用shutdown和close的区别，此时对端recv返回值的区别？





3、如何知道对端调用了close？





4、client调用close时，发送buf有数据，此时发送buf最后一块数据会被设置成FIN，此时server recv返回值是多少？client调用close时发送buf无数据，此时client仅发送FIN，此时server recv返回值是0，为什么《Linux多线程服务器编程》中8.7.1此时调用handleClose()，而前一种情况不调用？



5、LT和ET的POLLIN和POLLOUT触发条件分别是？

（1）LT的POLLIN触发条件：内核接收缓冲存在数据可读

（2）ET的POLLIN触发条件：内核接收缓冲有新数据到达

（3）LT的POLLOUT触发条件：内核发送缓冲存在空间可写新数据

（4）ET的POLLOUT触发条件（？）：内核发送缓冲在满载状态发送了数据（刚好空出可写新数据的空间）



5.1、全连接队列有连接未accept，此时添加监听端口的EPOLLET|EPOLLIN事件，会触发吗？



6、LT和ET触发的非阻塞send应该怎么写？

（1）LT的非阻塞send（方法1）：

可参照muduo的TCPConnection::sendInLoop()，需要设置回调handleWrite()，sendInLoop()无法一次性发送完毕时将socket的POLLOUT（writable事件）加入Poller，并保存未发送的数据，后续回调handleWrite()发送剩余数据。handleWrite()中**每次调用1次send**并检查数据是否发送完毕，若发送完毕要及时将socket的POLLOUT（writable事件）移出Poller。

《Linux多线程服务器编程》8.8中，作者认为send应该是ET的。LT的send需要控制好关注writable事件的时机，否则会导致busy loop（即不能一直关注writable事件），所以写起来比较麻烦。

（2）ET的非阻塞send（方法1）：

需要设置回调handleWrite()，handleWrite()中**循环调用send**，直到返回EAGAIN（或EWOULDBLOCK）或发送完毕。可以持续关注writable事件。如果handleWrite()在处理过程中发生改变，则需要在改变回调函数后，执行一次新的回调函数。

（为什么Nginx每次send后发现没有发送完毕时，都要再往epoll注册一次写？如ngx_http_discarded_request_body_handler。这个注册是CTL_MOD还是CTL_ADD还是修改ngx_event_t的active位？）



7、LT和ET的非阻塞recv应该怎么写？

（1）LT的非阻塞recv（方法1）：

可参照muduo的TCPConnection::handleRead()中的inputBuffer_.readFd()，需要设置回调handleRead()。handleRead()**每次调用1次readv**，《Linux多线程服务器编程》的8.7.2中提到LT保证数据不会丢失（我觉得应该也不会“卡死”在wait）、也能保证多个连接readv的公平性。同时还分析了通常一次readv就能读完全部数据的原因（即时延带宽积最多为几十KiB，一两个以太网frame）。

《Linux多线程服务器编程》8.8中，作者认为recv应该是LT模式的。

相比ET有啥好处？

（2）ET的非阻塞recv（方法1）：

设置回调handleRead()，handleRead()中**循环readv**（即至少两次recv），直至返回EAGAIN。如果处理过程中handleRead()发生改变（如Nginx中先用一个函数接收请求行再用一个函数接收请求头），则需要在改变回调函数后，执行一次新的回调函数。

（为什么Nginx每次recv后发现EAGAIN时，都要再往epoll注册一次读？如ngx_http_process_request_line。这个注册是CTL_MOD还是CTL_ADD还是修改ngx_event_t的active位？--不会注册，ngx_handle_read_event是尝试添加）



8、EPOLLONESHOT可以在LT模式下使用吗？意义何在？LT模式如何防止多个线程处理同一个socket？



9、同一个fd多次加入epoll/poll/select会发生什么？



10、epoll是如何实现的？为什么相比select和poll，epoll能处理百万级并发？

- 《深入理解Nginx》（陶辉）书上：基于红黑树管理事件，利用设备驱动触发回调函数，将事件从红黑树中提出到一个双向链表，epoll_wait直接访问查询双向链表（轮训查询？epoll_wait如何实现阻塞的？）。
- 如何实现进程睡眠？（linux0.12的linux/kernel/sched.c）
  - 多进程是需要schedule()和switch_to(next)来保证的，让进程睡眠本质上是不调度该进程，所以只要在schedule()函数中判断进程状态并跳过这种进程即可。
  - 所以当一个进程需要睡眠时，只需要把自己状态标记成不希望被调度，然后立刻调用schedule()让出CPU控制器即可。
  - 以上，正是sleep_on()干的活：需要睡眠的进程调用sleep_on，sleep_on会把当前进程状态标记为非就绪状态TASK_RUNNING（如需要显式唤醒的状态TASK_UNINTERRUPTIBLE）并把当前进程插入等待队列，然后调用schedule()。
- 如何实现进程唤醒？
  - 进程睡眠本质上是有特定标记的进程不被调度。唤醒进程就是修改标志位（如修改为就绪状态TASK_RUNNING）然后立刻调度schedule()。这也是内核wake_up()干的活。
  - （1）唤醒!=立刻执行。（2）唤醒后继续执行!=从sleep_on()返回，还需要符合等待队列FIFO语义（见sleep_on中调用schedule()的节点处代码）（3）唤醒==能被调度。
- select是如何实现的？（如何不用轮询也能直到非阻塞事件到来？）
  - 基于队列的睡眠：每个文件都有读写缓冲和读写等待队列。等待队列通过进程堆栈区的tmp（调用sleep_on()进入睡眠时）和old_task（调用select进入睡眠时）字段组成一个隐式链式队列。读写队列上进程的唤醒也是递归式的（即当前进程被唤醒，仅负责唤醒tmp或old_task指向的下一进程）。
  - select如何管理监听的文件：调用select的每个进程都会维护一个等待表wait_table，对于需要监听的文件描述符，等待表中都有一个entry指向该文件的读队列/写队列以及队列的old_task。
  - 如何检查文件是否阻塞：select通过函数check_XX检查对应文件的读缓冲（写缓冲）是否空（满），若空（满）则将当前进程头插入地添加到该文件的读队列（写队列）中。这也是add_wait()干的活儿。（所以文件会阻塞==文件的读缓冲空==文件的写缓冲满？）
  - 若发现有文件可读写，则用free_wait()尝试唤醒当前进程（同sleep_on()的schedule节点后的代码）；若发现无文件可读写，则标记当前进程进入睡眠（同sleep_on()的schedule节点前的代码）并调用schedule()让出控制权，并在schedule()返回后调用free_wait()并重新检查文件阻塞状态。
  - 猜测：文件的读写缓冲有数据变化时，会唤醒文件等待队列头部的进程
  - free_wait()：用于确保文件读写队列中比当前进程后加入的进程已被唤醒，并唤醒读写队列中当前进程的前一个进程（即old_task所指）。同sleep_on()中schedule()节点后的代码！
  - 为什么检查到有文件可读写时，返回count前要调用free_wait()：wait_table中的文件不是不可读写吗？为什么要唤醒这些文件读写队列的进程？
    - 应该是为了防止本次select产生的等待对后续进程状态产生影响。add_wait()实际上提供了唤醒进程的入口（等待队列有一个“当前进程”节点，等待队列被激活时“当前进程”就可能被唤醒），如果一次select结束前不通过free_wait()清空这些入口，则后续可能出现**假唤醒**。
    - 那为什么free_wait()不能只把自己从链式队列中剔除？为啥文件等待队列要用队列啊？
  - 为什么检查到无文件可读写时，schedule()返回后也要调用free_wait()：schedule()返回后表明当前进程获得CPU，表明此时可能（1）有某个文件的等待队列上的进程被唤醒了（而使本进程继续执行）。但**此时select无法知道是哪个文件可以读写了**，所以free_wait()中须要清空所有等待表后重新检查所有文件；（2）select超时了，开始第二次检查文件前，须要先清空第一次检查文件时对文件读写队列的修改，不然“过期当前进程”可能会在后续被唤醒（野进程）。
- poll如何实现？（相比select改进了啥？）
- epoll如何实现？（相比select改进了啥？）



11、socket通信中，send(2)和recv(2)都干了啥？内核如何实现的send和recv？（有无可能搞清楚它们的阻塞的工作机制？）

- linux中的文件：设备都被抽象为文件，而文件分为元信息+数据。元信息保留在磁盘的inode块和内存的inode_table[NR_INODE]中，数据保存在磁盘的数据区。即文件==inode+data。文件通过struct file file_table[NR_TABLE]保存，fd（文件描述符）就是访问入口。file_table[fd].inode就指向文件inode。系统调用sys_read()和sys_write()会根据file_table[fd].inode判断文件类型，然后调用对应类型文件的读写函数（如字符设备的rw_char()、一般文件的file_read()、管道文件的read_pipe()、块设备的block_read()）
  - socket属于什么类型的文件？
- 管道的阻塞读是怎么工作的（并不是轮询）：管道读端调用read_pipe()循环检查管道数据区长度，发现读完了管道数据时调用wak_up(&PIPE_WRITE_WAIT(\*inode))唤醒写端的文件等待队列（pipe返回两个fd！），并调用interruptible_sleep_on(&PIPE_READ_WAIT(*inode))将当前进程加入管道的读等待队列（但若发现没有写端会直接返回）以等待信号或写端调用write_pipe()时，其中的wake_up(&PIPE_READ_WAIT(\*inode))将其唤醒。
  - 为什么read_pipe()不用检查文件的f_flags判断文件阻塞状态？
    - 
- **猜测**：socket是个双工管道，当socket调用recv(2)阻塞后，**应该有一个负责从网络设备拉数据到socket内核接收缓冲的进程**，这个进程会负责wake_up(&SOCKET_READ_WAIT(\*inode))。
