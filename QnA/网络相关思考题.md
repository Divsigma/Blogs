1、一端close(socket)时，发送FIN还是RST？发送RST会有什么问题？如何解决？

内核根据**接收缓冲**是否有数据，决定发送FIN还是RST。

https://z.itpub.net/article/detail/E07055E62740F8C59E1649519A7F1A41

当一端接收缓冲还有数据时调用了close，此时它会清空接收缓冲并向对端发送RST，这可能导致**对端接收缓冲**中的数据无法被顺利读取（？），导致对端程序出错。

https://www.starduster.me/2019/07/06/socket-lingering-and-closing/

↑使用SO_LINGER似乎不能解决？↑而且内核的实现还会导致非阻塞socket阻塞？Nginx是怎么处理lingering close的？（lingering close处理是怎么一回事？）





2、一端调用shutdown和close的区别，此时对端recv返回值的区别？





3、如何知道对端调用了close？





4、client调用close时，发送buf有数据，此时发送buf最后一块数据会被设置成FIN，此时server recv返回值是多少？client调用close时发送buf无数据，此时client仅发送FIN，此时server recv返回值是0，为什么《Linux多线程服务器编程》中8.7.1此时调用handleClose()，而前一种情况不调用？



5、LT和ET的POLLIN和POLLOUT触发条件分别是？

（1）LT的POLLIN触发条件：内核接收缓冲存在数据可读

（2）ET的POLLIN触发条件：内核接收缓冲有新数据到达

（3）LT的POLLOUT触发条件：内核发送缓冲存在空间可写新数据

（4）ET的POLLOUT触发条件（？）：内核发送缓冲在满载状态发送了数据（刚好空出可写新数据的空间）



6、LT和ET触发的非阻塞send应该怎么写？

（1）LT的非阻塞send（方法1）：

可参照muduo的TCPConnection::sendInLoop()，需要设置回调handleWrite()，sendInLoop()无法一次性发送完毕时将socket的POLLOUT（writable事件）加入Poller，并保存未发送的数据，后续回调handleWrite()发送剩余数据。handleWrite()中**每次调用1次send**并检查数据是否发送完毕，若发送完毕要及时将socket的POLLOUT（writable事件）移出Poller。

《Linux多线程服务器编程》8.8中，作者认为send应该是ET的。LT的send需要控制好关注writable事件的时机，否则会导致busy loop（即不能一直关注writable事件），所以写起来比较麻烦。

（2）ET的非阻塞send（方法1）：

需要设置回调handleWrite()，handleWrite()中**循环调用send**，直到返回EAGAIN（或EWOULDBLOCK）或发送完毕。可以持续关注writable事件。如果handleWrite()在处理过程中发生改变，则需要在改变回调函数后，执行一次新的回调函数。

（为什么Nginx每次send后发现没有发送完毕时，都要再往epoll注册一次写？如ngx_http_discarded_request_body_handler。这个注册是CTL_MOD还是CTL_ADD还是修改ngx_event_t的active位？）



7、LT和ET的非阻塞recv应该怎么写？

（1）LT的非阻塞recv（方法1）：

可参照muduo的TCPConnection::handleRead()中的inputBuffer_.readFd()，需要设置回调handleRead()。handleRead()**每次调用1次readv**，《Linux多线程服务器编程》的8.7.2中提到LT保证数据不会丢失（我觉得应该也不会“卡死”在wait）、也能保证多个连接readv的公平性。同时还分析了通常一次readv就能读完全部数据的原因（即时延带宽积最多为几十KiB，一两个以太网frame）。

《Linux多线程服务器编程》8.8中，作者认为recv应该是LT模式的。

相比ET有啥好处？

（2）ET的非阻塞recv（方法1）：

设置回调handleRead()，handleRead()中**循环readv**（即至少两次recv），直至返回EAGAIN。如果处理过程中handleRead()发生改变（如Nginx中先用一个函数接收请求行再用一个函数接收请求头），则需要在改变回调函数后，执行一次新的回调函数。

（为什么Nginx每次recv后发现EAGAIN时，都要再往epoll注册一次读？如ngx_http_process_request_line。这个注册是CTL_MOD还是CTL_ADD还是修改ngx_event_t的active位？--不会注册，ngx_handle_read_event是尝试添加）



8、EPOLLONESHOT可以在LT模式下使用吗？意义何在？LT模式如何防止多个线程处理同一个socket？



9、同一个fd多次加入epoll/poll/select会发生什么？





























```c
/* http/ngx_http_request.c: 
 * static ssize_t ngx_http_read_request_header(ngx_http_request_t* r)
 */
ngx_connection_t *c = r->connection;
ngx_event_t *rev = c->read;
if (rev->ready) {
    n = c->recv(c, r->header_in->last, r->header_in->end - r->header_in->last);
} else {
    n = NGX_AGAIN;
}
if (n == NGX_AGAIN) {
    ...
    if (ngx_handle_read_event(rev, 0) != NGX_OK) {
        ngx_http_close_request(...);
        return NGX_ERROR;
    }
    return NGX_AGAIN;
}
if (n == 0) {
    ngx_log_error(NGX_LOG_INFO, c->log, 0, "client closed prematurely connection");
}

r->header_in->last += n;
return n;
```



```c
/* http/ngx_http_core_module.c: 
 * void ngx_http_handler(ngx_http_request_t* r)
 */

```





```c


/* http/ngx_http_request.c: 
 * ? ngx_http_init_connection(ngx_connection_t* c)
 */
rev = c->read;
rev->handler = ngx_http_init_request;
c->write->handler = ngx_http_empty_handler;
if (rev->ready) {
    ngx_http_init_request(rev);
    return;
}
// add timer

// add epoll event
if (ngx_handle_read_event(rev, 0) != NGX_OK) {
    // ..
}

/* event/ngx_event.c: 
 * ngx_int_t ngx_handle_read_event(ngx_event_t *rev, ngx_uint_t flags)
 */
// match ngx_event_flags with NGX_USE_CLEAR_EVENT(kqueue, epoll) || NGX_USE_LEVEL_EVENT(select, poll, /dev/poll)
if (ngx_event_flags & NGX_USE_CLEAR_EVENT) {
    if (!rev->active && !rev->ready) {
        if (ngx_add_event(rev, NGX_READ_EVENT, NGX_CLEAR_EVENT) == NGX_ERROR) {
            return NGX_ERROR;
        }
    }
    return NGX_OK;
} else if (...) {
    ...
}

/* event/ngx_event.h: 
 */
extern ngx_event_actions_t ngx_event_actions;
#define ngx_add_event	ngx_event_actions.add

#if (NGX_HAVE_KQUEUE)
   ...
#elif (NGX_HAVE_EPOLL)

#define NGX_READ_EVENT		EPOLLIN
#define NGX_WRITE_EVENT		EPOLLOUT
#define NGX_LEVEL_EVENT		0
#define NGX_CLEAR_EVENT		EPOLLET
#define NGX_ONESHOT_EVENT	0x70000000
    
#elif (NGX_HAVE_POLL)
    ...

/* event/modules/ngx_epoll_module.c: 
 */
       
static int ep = -1;
ngx_event_module_t ngx_epoll_module_ctx = {
    &epoll_name, 
    ..., 
    {
        ngx_epoll_add_event, /* add an event */
        ngx_epoll_del_event,
        ngx_epoll_add_event, /* enable an event */
        ngx_epoll_del_event,
        ngx_epoll_add_connection, /* add an connection */
        ...
    }
};

// [IN] static ngx_int_t ngx_epoll_init(ngx_cycle_t *cycle, ngx_msec_t timer)
if (ep == -1) {
    // 在许多Linux内核版本中，epoll_create不处理size参数，该参数不代表epoll能处理的最大事件个数
    ep = epoll_create(cycle->connection_n / 2);
    ...
}
...
ngx_event_actions = ngx_epoll_module_ctx.actions;

// [IN] static ngx_int_t ngx_epoll_add_event(ngx_event_t *ev, ngx_int_t event, ngx_uint_t flags)
uint32_t events, prev;
ngx_event_t *e;
ngx_connection_t *c;
struct epoll_event ee;

c = ev->data;
events = (uint32_t) event; // EPOLLIN 或者 EPOLLOUT
if (event == NGX_READ_EVENT) {
    e = c->write;
    prev = EPOLLOUT;
#if (NGX_READ_EVENT != EPOLLIN)
    events = EPOLLIN;
#endif
} else {
    e = c->read;
    prev = EPOLLIN;
#if (NGX_WRITE_EVENT != EPOLLOUT)
    events = EPOLLOUT;
}
// 若期望添加的event是NGX_READ_EVENT，则看对应连接的写事件是否active，反之看对应连接的读事件是否active。因为一个fd对应一对读写事件，所以若希望添加读事件时，写事件为active，则采用EPOLL_CTL_MOD往epoll中添加事件时不能丢掉写事件。所以此时events = NGX_READ_EVENT|EPOLLOUT|EPOLLET。（注意EPOLL_CTL_MOD是根据传入的结构体设置覆盖原来的设置！）
// Q：那同一个fd的读事件在修改handler后，多次传入ngx_epoll_add_event，而且对应的读事件不是active，岂不是会被EPOLL_CTL_ADD多次？
// A：epoll操作以文件描述符为单位，而一个文件描述符在Nginx中有读和写（2个事件），暂且互称为peer事件吧。这个if-else是用于判断peer事件是否已经在epoll的，如果在里头，就要把对端的epoll也注册。同一个fd的读事件在修改handler后，是调用ngx_handle_read_event往epoll添加的，这个接口对active==1的事件不会再调用ngx_epoll_add_event
if (e->active) {
    op = EPOLL_CTL_MOD;
    events |= prev;
} else {
    op = EPOLL_CTL_ADD;
}

ee.events = events | (uint32_t) flags; // [!active] EPOLLIN | EPOLLET, [active] EPOLLIN | EPOLLOUT | EPOLLET
ee.data.ptr = (void *)((uintptr_t)c | ev->instance);
...
if (epoll_ctl(ep, op, c->fd, &ee) == -1) {
    // log and return
    return NGX_ERROR;
}
ev->active = 1;
return NGX_OK;

// [IN] static ngx_int_t ngx_epoll_process_events(ngx_cycle_t *cycle, ngx_msec_t timer, ngx_uint_t flags)
if ((revents & (EPOLLERR|EPOLLHUP)) && (revents & (EPOLLIN|EPOLLOUT)) == 0)
{
    // 即如果revents不是同时没有ERR HUP IN OUT，则标记INOUT？
    // 其中会有2种情况，有异常但无INOUT的事件、无异常但有INOUT的事件，后者居然也要标记INOUT？
    // ↑优先级没搞清楚
    revents |= EPOLLIN|EPOLLOUT;
}
if ((revents & EPOLLIN) && rev->active) {
    // 对flags中无NGX_POST_EVENT和NGX_POST_THREAD_EVENTS的路径
    rev->ready = 1;
    rev->handler(rev);
}

```



